---
title: "example_solution.Rmd"
output: html_document
---

```{r setup, include=FALSE}

library(pacman)
p_load(tidyverse, lme4, cowplot)

```

## Exercise 1 --> Regression 1


1. Create a data-frame which containing the y and x data that you've used.
2. Run a linear model to check your results:  
2.1 The residuals should correspond to your errors
(non-squared). 
2.2 The estimates should correspond to your alpha and beta-values.
2.3 Is the R-squared value close to the one that you got?
2.4 Did you predict significance correctly?

```{r}

#specifying data: 
y = c(1, 4, 13, 18)
x = c(1, 2, 3, 4)

#put into dataframe: 
data <- data.frame(
  y, x
)

#model linear 
m1 <- lm(y ~ x, data)
summary(m1)


```

## Exercise 2 --> Regression 2 (more complexity): 

1. Run the model with a quadratic term. How should we specify this
model if we wanted to do the calculation by hand? For this you
can use the model.matrix() function which shows you the design matrix. 

2. Is this model better (higher R^2). Will a model with more predictors
always have a higher R^2?

3. now add a new (factorial) variable to your data-frame. 
You can call this variable gender. Set the value for the first
two rows to 1 and the value for the last two rows as 0. 

4. run a t-test on the data. How is the t-test specified (again
you can use model.matrix()). Is the t-test significant and is this
reasonable? Here you can draw and/or plot the model. If the
intercept is significant what does this mean?

5. how does the design matrix look for other more complicated 
models we could do? For instance, what about using both 
variables as predictors - how about an interaction effect?
Find out how the models are specified and which model is best. 

6. An ANOVA is simply a t-test with more groups. 
Make some fake data and find out how the design matrix 
for an ANOVA is specified. 

```{r}

#1: model quadratic 
m2 <- lm(y ~ x + I(x^2), data)
summary(m2)

#check model specification
model.matrix(~ x + I(x^2))

#adding a factor 
data <- data %>%
  mutate(gender = factor(c(1, 1, 0, 0)))

#t-test 
m3 <- lm(y ~ gender, data)
summary(m3)

model.matrix(~ data$gender)

#more complicated models
model.matrix(~ x * data$gender)
model.matrix(~ x:data$gender)
model.matrix(~ x + data$gender)

#ANOVA
data <- data %>%
  mutate(groups = factor(c(0, 1, 2, 3)))

model.matrix(~ data$groups)

```

## Exercise 3 (More data)

Load the data set "gender_attributes.csv" 
which is just some fake data that I generated. 
We are looking to predict/explain height (outcome)
by weight and/or gender (predictors/independent variables).
In this case male = 0, and female = 1. 

1. Check out the data: 
* Are the variables in a sensible/correct format?
* Are the variables normally distributed (why, why not?)
* Do some exploratory plotting of the data. 
NB: If you do multiple similar plots consider creating 
a function instead of writing redundant code. There are 
some trickies with writing tidy functions but it is well
worth getting the hang of. Check out for instance:

2. Modeling: 
* Discuss how the data could be modelled. 
* Run some models (using lm()) and discuss which model best describes the data. 
* If any effects are significant what does this mean? What does it mean, 
if the intercept is significant?
* Visualize the different models --> does it seem reasonable that
the best model is indeed best? 

```{r}

gender_att <- read_csv("gender_attributes.csv")

## format
str(gender_att) ## gender numeric.

gender_att <- gender_att %>%
  mutate(gender = as.factor(gender))

str(gender_att) ## gender now factor.

## normally distributed across groups?
plot_fun <- function(df, xvar) {
  
  plot <- ggplot(df, aes(x = {{xvar}})) + # wrapper 
    geom_density(fill = "cyan", alpha = 0.5) + 
    theme_minimal() 
  
  return(plot)
}

p1 <- plot_fun(gender_att, weight)
p2 <- plot_fun(gender_att, height)

plot_grid(p1, p2)

## normally distributed within groups?
plot_fun2 <- function(df, xvar, grouping) {
  
  plot <- ggplot(df, aes(x = {{xvar}}, fill = {{grouping}})) +
    geom_density(alpha = 0.5) + 
    theme_minimal()
  
  return(plot)
}

p3 <- plot_fun2(gender_att, weight, gender)
p4 <- plot_fun2(gender_att, height, gender)

plot_grid(p1, p2, p3, p4)

## The variables are approximately normally distributed within 
## each sex but not across. Here we get a bi-modal distribution. 
## Strongly suggests that gender is relevant to the phenomenon that
## we are modeling here. 

## exploratory plots ## 

#main effect of weight on height:
p1 <- ggplot(gender_att, aes(weight, height))+
  geom_point()+
  geom_smooth(method = "lm")


## mean data 
gender_mean <- gender_att %>%
  group_by(gender) %>%
  summarize(avg_height = mean(height),
            avg_weight = mean(weight))

head(gender_mean)
#main effect of gender on height:
p2 <- ggplot(gender_att, aes(gender, height, color = gender))+
  geom_jitter(width = 0.2) +
  geom_hline(aes(yintercept = avg_height, color = gender), 
             gender_mean)
p2

#both main effects (possible interaction):
p3 <- ggplot(gender_att, aes(weight, height, color = gender))+
  geom_point() +
  geom_smooth(method = "lm")

plot_grid(p1, p2, p3)

```

Appears clear that both main effects explain variance. 
Generally increasing weight leads to lower height (counterintuitively) ;) 
There is also a main effect of gender with males being taller than females.
However, both of these appear to mask the central interaction effect.
In fact for males we see that increased weight leads to increased height,
whereas the opposite pattern is true for females. 

Running models: 

```{r}
## run models: 
m1 <- lm(height ~ weight, data = gender_att)
summary(m1)

m2 <- lm(height ~ weight + gender, data = gender_att)
summary(m2) 

m3 <- lm(height ~ weight * gender, data = gender_att)
summary(m3)

```

The first model (only weight) suggests that the model which
includes weight does lead to a significantly better fit than
of the data than simply using the overall mean. 
The fact that the intercept is significant simply means that
the height of males (the intercept here because they are coded
as 0 whereas females are coded as 1) is significantly different from 0.
The model also suggests that the slope of weight 

The second model (both main effects) suggests
that a model of difference in group means (two-tailed t-test)
is a significantly better fit to our data than a model which
only considers the overall mean. After controlling for 
gender we now see that weight is in fact not a 
significant predictor.

The third model appears to suggest that both main effects 
and the interaction effect are significant. However, as we 
can clearly see from comparing earlier plots the main effects 
from the output, they do not 
really make sense. It suggests that gender1 (females) are 
on average much taller than men (which is not true) and 
that heavier weight is associated with being taller 
(which is also not true). We should throw the values of these out.
What the model really suggests is
that the interaction effect is better than a simple addition
of the two main effects (as in m2).

NB: how should we plot/visualize this?

```{r}

```

## Exercise 4 (embeddings)

Below I have loaded the fake word-embeddings. 

1. Compute the cosine similarity between 
1.1: man and woman.
1.2: man and king.
NB: look into the "geometry" library or 
compute the functions yourself:

2. Analogies are fun (and useful). For instance we all know that
man is to woman as king is to _____ (queen). 
Does our stupid word embeddings also know that?
How would you test that?

Think along the lines of: 
woman + king - man = queen  ---- or 
man + queen - woman = king 

Why should this work? Does it work?

3. Can you think of other analogies? Test them out. 


```{r}

install.packages("geometry")
library(geometry)

#read the data: 
embeddings <- read_csv("word_embeddings.csv") 
rownames(embeddings) <- c("gender", "royal", "age", "food") #throws error because it is bad style.

#function for norm: 
norm_vec <- function(x) {
  norm = sqrt(sum(x^2))
  return(norm)
  }

#cosine similarity between man and woman: 
dot_man_woman = dot(embeddings$man, embeddings$woman)
norm_man = norm_vec(embeddings$man)
norm_woman = norm_vec(embeddings$woman)

cosine_similarity = dot_man_woman / (norm_man * norm_woman)
cosine_similarity

#cosine similarity between man and king: 
dot_man_king = dot(embeddings$man, embeddings$king)
norm_king = norm_vec(embeddings$king)

cosine_similarity2 = dot_man_king / (norm_man * norm_king)
cosine_similarity2

#2: is queen to king as woman is to man?
new_vector = embeddings$woman + embeddings$king - embeddings$man

#cosine similarity between our new vector and queen. 
dot_new_vector_queen = dot(embeddings$queen, new_vector)
norm_new_vect = norm_vec(new_vector)
norm_queen = norm_vec(embeddings$queen)

cosine_similarity3 = dot_new_vector_queen / (norm_new_vect * norm_queen)
cosine_similarity3 #almost perfect. 

```

## Exercise 5 (PCA) 

Navigate to this link: 
https://www.datacamp.com/community/tutorials/pca-analysis-r?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=aud-763347114660:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1005278&gclid=Cj0KCQjwxNT8BRD9ARIsAJ8S5xbEDnmO1D8NfpRcz7SikOBnG21xLEJi1Pv6PpQ9Uvj81RIE_9vdpscaAkxLEALw_wcB 

1. It walks you through a basic PCA analysis in R on the mtcars 
(build in) data-set. I have pasted it in below. 
Adapt the analysis for some new data (of your choosing). 

2. Does the two principal components in your analysis seem
reasonable (i.e. do the clusters make sense and not overlap too much?). 
Which parameters/predictors contribute most to the most important principal components?

3. Does it seem like we could drop any predictors/parameters?

4. Why is PCA useful? 


```{r}

## load mtcars data set: 
mtcars <- mtcars

## select numeric variables: 
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)

## inspect the data: 
summary(mtcars.pca)
str(mtcars.pca)

## plot the data
ggbiplot(mtcars.pca, labels=rownames(mtcars))

## Not sure where the tutorial gets this information: 
mtcars.country <- c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))

## with ellipses marking the groups by country: 
ggbiplot(mtcars.pca,ellipse=TRUE,  labels=rownames(mtcars), groups=mtcars.country)

## with "worse" principal components (explaining less variance): 
ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(mtcars), groups=mtcars.country)

## what is the recipe for each PC?
mtcars.pca$rotation

```


